{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hw9PbWOYxrZ8"
   },
   "source": [
    "## Steps to fine tune model\n",
    "- Select and load model\n",
    "- Select and preprocess dataset (train/eval split, tokenize)\n",
    "- Define quantization or adaptation before fine-tuning for efficiency\n",
    "- Tune and evaluate model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqFGf9Ak9jFT"
   },
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZYRbi8UM1pa5",
    "outputId": "52131dcf-b80f-4a5e-889f-404c2e9bf34a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/280.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/280.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/190.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q accelerate transformers peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4b__4hz3E5c",
    "outputId": "fefc7eb7-e378-421b-edf8-0b25036ff823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets scipy ipywidgets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R2C59WTv68VR",
    "outputId": "e43e8cfc-2fe7-4cc4-b98e-86bda2258134"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IqvB8GRjx9kC"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 775
    },
    "id": "SlG_g0CzyxqI",
    "outputId": "ac9989de-ed74-4eb2-bbc9-40b49a3da90b"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like microsoft/phi-2 is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 503 Server Error: Service Temporarily Unavailable for url: https://huggingface.co/microsoft/phi-2/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1238\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1239\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1630\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    386\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;31m# as well (request id and/or server error message)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 503 Server Error: Service Temporarily Unavailable for url: https://huggingface.co/microsoft/phi-2/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    386\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1370\u001b[0m             \u001b[0;31m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             raise LocalEntryNotFoundError(\n\u001b[0m\u001b[1;32m   1372\u001b[0m                 \u001b[0;34m\"An error happened while trying to locate the file on the Hub and we cannot find the requested files\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-515767543196>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", device_map=\"auto\",\n\u001b[0m\u001b[1;32m     18\u001b[0m                                             \u001b[0;31m#  quantization_config=bnb_config,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                             \u001b[0;31m#  torch_dtype=torch.float16, load_in_8bit=True,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quantization_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    527\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0moriginal_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    690\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_missing_entries\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_connection_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0;34mf\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this file, couldn't find it in the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;34mf\" cached files and it looks like {path_or_repo_id} is not the path to a directory containing a file named\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like microsoft/phi-2 is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    # bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", device_map=\"auto\",\n",
    "                                            #  quantization_config=bnb_config,\n",
    "                                            #  torch_dtype=torch.float16, load_in_8bit=True,\n",
    "                                             token=access_token)\n",
    "# tokenizer = AutoTokenizer.from_pretrai\\ned(\"microsoft/phi-2\")#, trust_remote_code=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/phi-2\",\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    "    use_fast=False\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vjsBRZo8zvjQ",
    "outputId": "b70769da-814f-4316-89da-ce2d35a6ae2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2779683840"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available(), model.device)\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gk8GS4gWA5_C",
    "outputId": "e7e1b282-93b1-49ff-e07a-a349bf786eae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2779683840"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params\n",
    "count_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "G3iNGsoQtA_E"
   },
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0P-Y9Sw7Odu",
    "outputId": "bb0dc033-56f7-45fa-b982-8d317dbcfee0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 28 17:29:54 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   71C    P0              33W /  70W |  10851MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ZRUreX27p_F",
    "outputId": "01262e61-bef2-41c3-9f4c-7bc77bddb6cb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n",
      "    Give me an email-only content calendar for activating users that haven't used my service in more than 3 months. \n",
      "    The service is an online marketplace for prospective home buyers to find houses and apartments in the Netherlands. \n",
      "    Instructions: for each content idea provided, give me why this is something that makes sense for activating dormant users, \n",
      "    and how to adjust the content strategy depending on whether it has been successful at activating the user after each month.\n",
      "\n",
      "1. A monthly newsletter with a list of the most popular houses and apartments in the Netherlands.\n",
      "2. A weekly blog post with tips on how to find the perfect home in the Netherlands.\n",
      "3. A monthly webinar with a real estate expert discussing the current housing market in the Netherlands.\n",
      "4. A monthly social media campaign featuring user-generated content from people who have found their dream home in the Netherlands.\n",
      "5. A monthly email with a personalized recommendation for a house or apartment based on the user's preferences.\n",
      "\n",
      "Question: What is the email-only content calendar for activating users that haven't used the service in more than 3 months?\n",
      "\n",
      "\n",
      "\n",
      "First, we need to identify the content that is most likely to engage dormant users. This can be done by analyzing the user behavior data and identifying the content that has been successful in the past.\n",
      "\n",
      "Next, we need to adjust the content strategy depending on whether it has been successful at activating the user after each month. For example, if the monthly newsletter with a list of the most popular houses and apartments in the Netherlands has been successful in the past, we can continue to use this content. However, if it has not been successful, we need to come up with a new strategy.\n",
      "\n",
      "We can use inductive logic to predict which content will be successful in the future based on past data. For example, if the weekly blog post with tips on how to find the perfect home in the Netherlands has been successful in the past, we can predict that it will be successful in the future as well.\n",
      "\n",
      "We can also use deductive logic to eliminate content that has not been successful in the past. For example, if the monthly webinar with a real estate expert discussing the current housing market in the Netherlands has not been successful in the past, we can eliminate this content from our email-only content calendar.\n",
      "\n",
      "We can use proof by exhaustion to test all possible combinations of content to\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"\"\"\n",
    "  Give me an email-only content calendar for activating users that haven't used my service in more than 3 months.\n",
    "  The service is an online marketplace for prospective home buyers to find houses and apartments in the Netherlands.\n",
    "  Instructions: for each content idea provided, give me why this is something that makes sense for activating dormant users,\n",
    "  and how to adjust the content strategy depending on whether it has been successful at activating the user after each month.\n",
    "\"\"\",\n",
    "                   return_tensors=\"pt\", return_attention_mask=False).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=500)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "d4_e7WOi2p6s"
   },
   "outputs": [],
   "source": [
    "# with open('finetune-emails.txt', 'r') as file:\n",
    "#     text = ''.join([row for row in file.readlines() if row != '\\n'])\n",
    "# with open('sadyr_preprocessed.txt', 'w') as file:\n",
    "#     file.write(text)\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(path='text', data_files='finetune-emails.txt', split='train')\n",
    "dataset = dataset.train_test_split(train_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1YGB8EZ9AYY7",
    "outputId": "c53417bc-105a-4989-94e0-ed78798a2e98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 30132\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 3349\n",
       " }))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "1a66559c14b14e86ac3e38581c87d773",
      "bf091b6addf846bf881bdddcaa1d292f",
      "9ff5bf96034e4982be9a296658e18b28",
      "08d4c699bdac443cb2696a9016bab802",
      "48c3a62fa47a404ea92e1c17fc7b50eb",
      "27b71e0556d84b62ab024d6df25f5422",
      "72bb036a5c264621b85589199debd8cf",
      "c2c7d284450841259db296c370886281",
      "36f6e9448580497bbfb47b2dde1e2bf4",
      "cef43065c0cd4a58b04212cefd61106c",
      "c200736bc07e4c35a95e8a923e225d28",
      "5f462014cb824dea9bec2ae36f7f01ad",
      "1f2c71bf1e9348518fae1c087ffe1b72",
      "295cc2482da0441080cc3404f488d09c",
      "3bcbf7ccbecb4b6aab73017ec7c6da05",
      "661fa7e195d844a482a212be4c4ee79c",
      "905184dcef1f4df1b8568a6aa8d1882c",
      "cb334193ef184a80ac76acafc42e9e15",
      "05849d81ea3c476399fa16ed04df701e",
      "11c77b811e0745569b4dc40685f71b6e",
      "75756ad6cd8c4556beb9bb1f36da3c84",
      "5d792d0310e94385bd150658adf60c34"
     ]
    },
    "id": "rR2_x3G8ALfi",
    "outputId": "5fe0b580-107d-41b6-9a23-22b8f1a4848c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a66559c14b14e86ac3e38581c87d773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30132 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f462014cb824dea9bec2ae36f7f01ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_and_tokenize_prompt = lambda prompt: tokenizer(prompt['text'])\n",
    "\n",
    "tokenized_train_dataset = dataset['train'].map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = dataset['test'].map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "MC7quQviA-3G",
    "outputId": "12c943f5-be52-42fc-ea89-68765bb521ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33481\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "KE58ylPYA_h3"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gy8GfYh9IQn9",
    "outputId": "9b3d5eec-3b76-417a-b449-46dbbd1510a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiForCausalLM(\n",
      "  (model): PhiModel(\n",
      "    (embed_tokens): Embedding(51200, 2560)\n",
      "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x PhiDecoderLayer(\n",
      "        (self_attn): PhiAttention(\n",
      "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          (rotary_emb): PhiRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): PhiMLP(\n",
      "          (activation_fn): NewGELUActivation()\n",
      "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "        )\n",
      "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "L-BDW0myBNog"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        # \"dense\",\n",
    "        \"fc1\",\n",
    "        \"fc2\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "# print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "1ysNNsIFBTQV",
    "outputId": "89f5c35d-d9eb-41cd-9511-4b02e30f2cf0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'trainable params: 1.4864842795940476'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'trainable params: {100 * count_trainable_parameters(model) / model.num_parameters()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OsOydCvMDjIT"
   },
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)\n",
    "model = accelerator.prepare_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "id": "b-zP3fSsBfgN",
    "outputId": "ac5c246e-8e5f-4809-95ab-4e66fa9b268c"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='5652' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  44/5652 07:52 < 17:32:17, 0.09 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='101' max='5652' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 101/5652 17:30 < 16:22:04, 0.09 it/s, Epoch 0.05/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.11 GiB. GPU 0 has a total capacty of 14.75 GiB of which 775.06 MiB is free. Process 75744 has 13.72 GiB memory in use. Of the allocated memory 12.72 GiB is allocated by PyTorch, and 887.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4e0e9f2b9bfd>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# silence the warnings. Please re-enable for inference!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1539\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1540\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1868\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1869\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1871\u001b[0m                 if (\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2771\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2772\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2774\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2793\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2794\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2795\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2796\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2797\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOLY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m             return self.base_model(\n\u001b[0m\u001b[1;32m   1092\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/phi/modeling_phi.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.11 GiB. GPU 0 has a total capacty of 14.75 GiB of which 775.06 MiB is free. Process 75744 has 13.72 GiB memory in use. Of the allocated memory 12.72 GiB is allocated by PyTorch, and 887.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "\n",
    "\n",
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"emails\"\n",
    "base_model_name = \"phi-2\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=1,\n",
    "        # per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        # max_steps=500,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "        # bf16=True,\n",
    "        # fp16=True,\n",
    "        # optim=\"paged_adamw_8bit\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        logging_steps=25,              # When to start reporting loss\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=1000,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=1000,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        # report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        # run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLrn4htyCTEm"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer('Who is Sadyr Zhaparov?',\n",
    "                   return_tensors=\"pt\", return_attention_mask=False).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LeYIdS61GF9q"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05849d81ea3c476399fa16ed04df701e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08d4c699bdac443cb2696a9016bab802": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cef43065c0cd4a58b04212cefd61106c",
      "placeholder": "​",
      "style": "IPY_MODEL_c200736bc07e4c35a95e8a923e225d28",
      "value": " 30132/30132 [00:15&lt;00:00, 2163.66 examples/s]"
     }
    },
    "11c77b811e0745569b4dc40685f71b6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1a66559c14b14e86ac3e38581c87d773": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bf091b6addf846bf881bdddcaa1d292f",
       "IPY_MODEL_9ff5bf96034e4982be9a296658e18b28",
       "IPY_MODEL_08d4c699bdac443cb2696a9016bab802"
      ],
      "layout": "IPY_MODEL_48c3a62fa47a404ea92e1c17fc7b50eb"
     }
    },
    "1f2c71bf1e9348518fae1c087ffe1b72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_905184dcef1f4df1b8568a6aa8d1882c",
      "placeholder": "​",
      "style": "IPY_MODEL_cb334193ef184a80ac76acafc42e9e15",
      "value": "Map: 100%"
     }
    },
    "27b71e0556d84b62ab024d6df25f5422": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "295cc2482da0441080cc3404f488d09c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05849d81ea3c476399fa16ed04df701e",
      "max": 3349,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_11c77b811e0745569b4dc40685f71b6e",
      "value": 3349
     }
    },
    "36f6e9448580497bbfb47b2dde1e2bf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3bcbf7ccbecb4b6aab73017ec7c6da05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75756ad6cd8c4556beb9bb1f36da3c84",
      "placeholder": "​",
      "style": "IPY_MODEL_5d792d0310e94385bd150658adf60c34",
      "value": " 3349/3349 [00:01&lt;00:00, 1912.59 examples/s]"
     }
    },
    "48c3a62fa47a404ea92e1c17fc7b50eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d792d0310e94385bd150658adf60c34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5f462014cb824dea9bec2ae36f7f01ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1f2c71bf1e9348518fae1c087ffe1b72",
       "IPY_MODEL_295cc2482da0441080cc3404f488d09c",
       "IPY_MODEL_3bcbf7ccbecb4b6aab73017ec7c6da05"
      ],
      "layout": "IPY_MODEL_661fa7e195d844a482a212be4c4ee79c"
     }
    },
    "661fa7e195d844a482a212be4c4ee79c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72bb036a5c264621b85589199debd8cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75756ad6cd8c4556beb9bb1f36da3c84": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "905184dcef1f4df1b8568a6aa8d1882c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ff5bf96034e4982be9a296658e18b28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2c7d284450841259db296c370886281",
      "max": 30132,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_36f6e9448580497bbfb47b2dde1e2bf4",
      "value": 30132
     }
    },
    "bf091b6addf846bf881bdddcaa1d292f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27b71e0556d84b62ab024d6df25f5422",
      "placeholder": "​",
      "style": "IPY_MODEL_72bb036a5c264621b85589199debd8cf",
      "value": "Map: 100%"
     }
    },
    "c200736bc07e4c35a95e8a923e225d28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c2c7d284450841259db296c370886281": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb334193ef184a80ac76acafc42e9e15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cef43065c0cd4a58b04212cefd61106c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
